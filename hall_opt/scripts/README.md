
# IN PROGRESS


## Libraries:

### import statsmodels.tools.numdiff as smnd

https://www.statsmodels.org/stable/generated/statsmodels.tools.numdiff.approx_hess1.html

# MAP Optimization Behavior and Diagnostics

## Why MAP is a Good Pre-step for MCMC

**Finding a High-Probability Region**:  
The MAP estimate represents the mode (peak) of the posterior distribution.  
Starting MCMC chains at or near this mode helps ensure they begin exploring in a region where the target probability is high.

Reference:  
https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation

---

## Why the "MAP failed" Error When `max_iter` Is Reached

`max_iter` is a stopping condition, not necessarily a success condition.  
When `scipy.optimize.minimize` stops because it hit the `maxiter` limit, it means it didn't converge according to its other criteria (like tolerances `fatol`, `xatol`, gradient norms, etc.) within the allowed number of iterations.

Because it didn't converge "naturally" by meeting the tolerance criteria, the `result.success` flag returned by `minimize` will be set to `False`.

The script checks `if result.success:` after the `minimize` call. Since `result.success` is `False`, code proceeds to the `else:` block and prints `[ERROR] MAP optimization failed.`

**Conclusion**: This is the expected behavior.  
Reaching the iteration limit without meeting convergence criteria is considered an unsuccessful optimization run by SciPy. The error message is generated by code's handling of the unsuccessful result flag.

References:  
https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html  
https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize  
https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html

---

## Why 9 Callback Iterations When `max_iter=5`

The definition of an "iteration" counted by `maxiter` within the optimization algorithm (especially for gradient-based methods like SLSQP, BFGS, L-BFGS-B, Nelder-Mead (not gradient-based)) doesn't always correspond 1-to-1 with calls to the callback function.

**Major Iterations vs. Callback Calls**:  
`maxiter` limits the number of major steps of the algorithm (e.g., updating the main guess based on gradient and line search).

**Function Evaluations / Line Search**:  
Within a single major iteration, the algorithm (especially gradient-based ones) might perform a "line search" to find the optimal step size along a calculated direction.  
This line search can involve multiple evaluations of the objective function (`neg_log_posterior_with_penalty`).

**Callback Timing**:  
The callback function might be called more frequently than just once per major iteration.  
It could be called after each function evaluation during the line search, or at other intermediate points depending on the specific algorithm (method) chosen.

---

# Parameters QA

## What is `x`?

`x` represents the final parameter values found by the optimization algorithm when it stopped.
(see scipy documentation reference from above)
In our specific case:  
Since `run_map_workflow` function optimizes the parameters in log space (passed `initial_guess` in log space and the objective function `neg_log_posterior_with_penalty` operates on `c_log`), the values in `x` correspond to `[c1_log, alpha_log]`.

**Example**:
- `x[0]` (-0.270...) is the final `c1_log` value.
- `x[1]` (0.425...) is the final `alpha_log` value.

**Significance**:  
This is the "best guess" for the parameters that minimize the objective function (`neg_log_posterior_with_penalty`) that the optimizer could find within the allowed 5 iterations.  
Even though `success` is `False` (because it didn't converge based on tolerance), `x` still holds the location of the minimum function value (`fun`) encountered before stopping.

---

## What is `final_simplex`?

Specific to the `'Nelder-Mead'` optimization algorithm.  
Nelder-Mead works by maintaining a geometric shape called a simplex in the parameter space.  
For a problem with N parameters, the simplex has N+1 vertices (points).  
In our 2-parameter case (`c1_log`, `alpha_log`), the simplex is a triangle (3 vertices). The algorithm iteratively modifies this simplex (reflecting, expanding, contracting) based on the function values at its vertices to move towards a minimum.

**`final_simplex` Contents**:
- **"vertices"**: This is a list containing the coordinates (`[c1_log, alpha_log]`) of each of the 3 vertices of the triangle when the optimization terminated.  
  Notice that the first vertex listed (example: `[-0.270..., 0.425...]`) is identical to the best point found (`x`).  
  This is typical, as `x` usually corresponds to the vertex with the lowest function value in the final simplex.

- **"values"**: This is a list containing the objective function value (`fun`) evaluated at each corresponding vertex listed in `"vertices"`.  

**Significance**:  
It gives an insight into where the Nelder-Mead algorithm was exploring right at the end.  
It shows the spread of points it was considering.

# MCMC 

#### The acceptance rate is automatically tracked in the MetropolisHastings base class through:

python
``` 
    self.accept_num  # Number of accepted proposals
    self.num_steps   # Total steps taken
```
And there's even a method provided:

``` 
  def accept_ratio(self):
      return self.accept_num / self.num_steps
```
 